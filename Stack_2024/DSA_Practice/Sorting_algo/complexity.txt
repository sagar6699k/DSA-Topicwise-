1)**Merge Sort Algorithm:**

Merge sort is a divide-and-conquer algorithm that recursively divides 
an array into two halves, sorts each half, and then merges the sorted halves. 
Here's the analysis of time and space complexity:

**Time Complexity: O(n log n)**
- The algorithm divides the array in half log n times (where n is the size of the array).
- For each level of recursion, it performs linear time work during the merge step.
- The time complexity is O(n log n) in the worst, average, and best cases.

**Space Complexity: O(n)**
- Merge sort typically requires additional space for the temporary storage of the merged subarrays.
- The space required for the temporary arrays during the merge process is proportional to the 
size of the input array.
- The space complexity is O(n) as it creates temporary arrays of size n.


2)**Quicksort Algorithm:**

Quicksort is a divide-and-conquer sorting algorithm that works by 
selecting a 'pivot' element from the array and partitioning the other 
elements into two sub-arrays according to whether they are less than or greater 
than the pivot. The sub-arrays are then sorted recursively. 
Here's the analysis of time and space complexity:

**Time Complexity:**
- **Worst Case:** O(n^2)
  - The worst-case occurs when the pivot is consistently chosen poorly, 
  leading to unbalanced partitions. For example, if the pivot is always the 
  smallest or largest element.
- **Average Case:** O(n log n)
  - The average-case time complexity is much better than the worst case, 
  as the partitioning process tends to create balanced sub-arrays.
- **Best Case:** O(n log n)
  - The best-case occurs when the pivot consistently divides the array into 
  two equal halves.

Quicksort is known for its efficiency and is often faster in practice than 
other O(n log n) algorithms like merge sort, despite its potential for a 
quadratic worst-case scenario.

**Space Complexity:**
- Quicksort is typically an in-place sorting algorithm. 
Most implementations sort the array in situ and do not require additional 
memory proportional to the input size.
- However, the space complexity can vary depending on the implementation. 
Recursive implementations may have a logarithmic stack space usage in the 
average case.
- Some implementations use techniques like tail call optimization 
or iterative methods to reduce the stack space usage.

In summary, quicksort has an average-case and best-case 
time complexity of O(n log n) and a worst-case time complexity of O(n^2). 
Its space complexity can be O(log n) on average, 
but this can vary depending on the implementation. 
Despite the potential for a quadratic worst case, quicksort is often used in 
practice due to its average-case efficiency and low constant factors.

3)**Bubble Sort Algorithm:**

Bubble sort is a simple sorting algorithm that repeatedly steps through the list,
compares adjacent elements, and swaps them if they are in the wrong order.
The pass through the list is repeated until the list is sorted.
Here's the analysis of time and space complexity:

**Time Complexity:**
- **Worst Case:** O(n^2)
  - The worst-case occurs when the array is in reverse order, 
  and in each pass, you have to swap every element.
- **Average Case:** O(n^2)
- **Best Case:** O(n)
  - The best-case occurs when the array is already sorted, 
  and no swaps are needed.

Bubble sort has a nested loop structure, and the number of comparisons 
and swaps is proportional to n in the worst and average cases.

**Space Complexity:**
- Bubble sort is an in-place sorting algorithm, meaning it doesn't require
 additional memory space proportional to the input size.
- The space complexity is O(1), as it uses only a constant amount of 
extra space regardless of the input size.


4)**Selection Sort Algorithm:**

Selection sort is another simple sorting algorithm that divides the input 
list into two parts: a sorted and an unsorted region. The algorithm repeatedly 
selects the smallest (or largest, depending on the ordering) element 
from the unsorted region and swaps it with the first element of the 
unsorted region. Here's the analysis of time and space complexity:

**Time Complexity:**
- **Worst Case:** O(n^2)
  - The worst-case occurs when the array is in reverse order, and in each pass,
   you have to find the minimum element and swap it.
- **Average Case:** O(n^2)
- **Best Case:** O(n^2)
  - The best-case occurs when the array is already sorted, but the algorithm 
  still needs to perform comparisons to determine the minimum element.

Similar to bubble sort, selection sort involves nested loops, 
and the number of comparisons and swaps is proportional to n in the worst 
and average cases.

**Space Complexity:**
- Selection sort is an in-place sorting algorithm, meaning it doesn't require 
additional memory space proportional to the input size.
- The space complexity is O(1), as it uses only a constant amount of extra 
space regardless of the input size.
